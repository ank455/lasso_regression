{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d5d11-d734-4e16-8302-fd17f4d0749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 1\n",
    "Lasso Regression, also known as L1 regularization, is a regression technique used for feature selection and regularization in statistical models. It is particularly useful when dealing with high-dimensional data or when there is a need to select the most relevant features from a set of predictors.\n",
    "\n",
    "In traditional linear regression, the goal is to fit a linear model that minimizes the sum of squared residuals. However, in Lasso Regression, an additional penalty term is added to the loss function, which is the sum of the absolute values of the regression coefficients multiplied by a tuning parameter (λ). The objective function of Lasso Regression can be expressed as:\n",
    "\n",
    "Loss function + λ * (sum of absolute values of regression coefficients)\n",
    "\n",
    "The addition of the penalty term introduces a constraint on the magnitude of the coefficients. As a result, Lasso Regression tends to shrink the coefficients of less relevant features to zero, effectively performing feature selection by eliminating irrelevant predictors. This is in contrast to other regression techniques like Ridge Regression, which uses L2 regularization and shrinks the coefficients towards zero without eliminating them entirely.\n",
    "\n",
    "The key difference between Lasso Regression and other regression techniques, such as Ridge Regression, lies in the type of regularization applied. Lasso Regression's L1 regularization promotes sparsity in the coefficient matrix, leading to a solution where only a subset of the predictors have non-zero coefficients. On the other hand, Ridge Regression's L2 regularization encourages smaller but non-zero coefficients for all predictors.\n",
    "\n",
    "The sparsity-inducing property of Lasso Regression makes it particularly useful in situations where there are many predictors, and only a few of them are expected to have a significant impact on the target variable. By automatically selecting relevant features and shrinking the coefficients of irrelevant ones, Lasso Regression can provide more interpretable models and help avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4d609-e03c-4cdc-885b-c23d9f35cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 2\n",
    "The main advantage of using lasso regression for feature selection.\n",
    "(1)Automatic feature selection\n",
    "(2)Interpretability\n",
    "(3)Reduced overfitting\n",
    "(4)Improved computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd718da-79b2-4fa1-8aa7-43ec50ea8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 3\n",
    "The number of non-zero coefficients can be used to evaluate the effectiveness of the regularization and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e2ad4-af3d-4218-8f08-f045e2ee95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 4\n",
    "In Lasso Regression, there are two main tuning parameters that can be adjusted to control the model's behavior and performance:\n",
    "\n",
    "(1)Lambda (λ) / Alpha (α)\n",
    "(2)Standardization / Scaling\n",
    "\n",
    "Choosing the appropriate values for the tuning parameters is crucial in achieving the desired balance between model complexity, feature selection, and performance. This selection is typically done using techniques like cross-validation, where different values of λ or α are tested, and the one that optimizes a chosen performance metric on a validation set is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29dd5ba-37ca-4192-bf47-e768649aaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 5\n",
    "No,Lasso Regression be used for non-linear regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ab15c-fb16-46ba-9728-68a38954540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 6\n",
    "The lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e5e28-2809-4dda-bdd7-783df54b48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 7\n",
    "Lasso Regression method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939c68a-cae2-4b44-a810-1a646f2a4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans 8\n",
    "The best cross-validation score is obtained for the 0.4 value of lambda. This is optimal value of lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
